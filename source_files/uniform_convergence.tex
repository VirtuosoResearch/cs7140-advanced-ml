\section{Uniform convergence and generalization}

Recall that we have introduced the empirical risk and the expected risk of a hypothesis (denoted by $L(h)$ and $\hat L(h)$) for some $h$ in a hypothesis class $\cH$.
Suppose we minimize the empirical risk to get $\hat h_{\erm}$. Two questions:
\begin{itemize}
    \item Generalization gap: how does the expected and empirical risks compare for ERM, i.e., $L(\hat h_{\erm}) - \hat L(\hat h_{\erm})$? This is called the \hl{generalization gap}.

    \item Excess risk: how well does ERM do with respect to the best possible hypothesis in the hypothesis class, i.e., $L(\hat h_{\erm}) - \min_{h \in \cH} L(h)$?
    This is also called the \hl{excess risk}.
\end{itemize}

A particularly fruitful framework for analyzing learning algorithms is the probably approximately correct (PAC) framework \citep{valiant1984theory}:

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.1\textwidth,
    rightmargin=0.1\textwidth]
    A learning algorithm $A$ PAC learns a hypothesis class $\cH$ if
    \begin{enumerate}[label=\alph*)]
        \item For any distribution $\bP^{\star}$ supported over $\cX \times \cY$, and any $\epsilon > 0$, $\delta > 0$
        \item Upon taking $n$ I.I.D. samples from $\bP^{\star}$, $A$ produces an output $\hat h \in \cH$ such that with probability at least $1 - \delta$ (over the randomness of the samples)
        \[ L(\hat h) - \hat L(\hat h) \le \epsilon \]
        \item Further, $A$ runs in time polynomial in $n, d, \epsilon^{-1}, \delta^{-1}$ (where $d$ is the dimension of the input)
    \end{enumerate}
\end{mdframed}

\paragraph{Remark:} Notice that the running time complexity places a bound on the sample complexity as well. We will assume that the empirical risk minimizer can be computed efficiently.
For instance, think of a large neural network whose training loss can be efficiently reduced to reach zero using stochastic gradient descent

\subsection{Learning a realizable, finite hypothesis class (Lecture 2)}

The ERM framework is very general -- we now give a concrete example to illustrate some basic results.

\paragraph{Assumptions (realizable, finite hypothesis):}
i) The size of the hypothesis space, $\cH$, is finite;
ii) There exists a hypothesis $h^{\star} \in \cH$ such that $h^{\star}$ achieves perfect performance, i.e., \[ L(h^{\star}) = \exarg{(x, y) \in \bP^{\star}}{\ell( h^{\star}(x), y )} = 0. \]

Under these assumptions, we shall prove the following property of ERM:

\begin{mdframed}[
    roundcorner=10pt,
    leftmargin=0.1\textwidth,
    rightmargin=0.1\textwidth]
    Under the above assumptions, with probability $1 - \delta$,
    \[ L(\hat h_{\erm}) \le \frac {\log(\abs{\cH}) + \log(\delta^{-1})} {n} \]
\end{mdframed}

In particular, to reduce the expected risk below $\epsilon$, we want $n \ge \frac {\log(\abs{\cH}) + \log(\delta^{-1})} {\epsilon}$.
Remarks:
\begin{itemize}
    \item The excess risk only grows logarithmically with the size of the hypothesis class, so we affort to use an exponentially large hypothesis space.

    \item The result is independent of $\bP^{\star}$. This is nice because typically we don’t know the true distribution.
\end{itemize}

\begin{proof}
    We’d like to upper bound the probability of the bad event that $L(\hat h_{\erm}) > \epsilon$:
    \begin{enumerate}[label=\alph*)]
        \item Let $B \subseteq \cH$ be the set of bad hypotheses: $\set{B \in \cH : L(h) > \epsilon}$

        \item We can rewrite our goal as upper bounding the probability of selecting a bad hypothesis $\Pr[L(\hat h_{\erm}) > \epsilon] = \Pr[\hat h_{\erm} \in B]$

        \item Recall the empirical risk of ERM is always zero because at least $\hat L(h^{\star}) = 0$

        \item Hence for any “bad” hypothesis in $B$, they must have zero empirical risk
        \[ \Pr[\hat h_{\erm} \in B] \le \Pr[\exists h \in B : \hat L(h) = 0] \]

        \item Now we shall deal with the above in two steps.
        First, bound $\Pr[ \hat L(h) = 0 ]$ for a fixed $h \in B$.

        Notice that on a random example from $\cP^{\star}$, the accuracy of $h$ should be $1 - L(h)$.
        Since the training data is drawn IID from $\cP^{\star}$, and $L(h) \ge \epsilon$ for any $h \in B$, we get that
        \[ \Pr[ \hat L(h) = 0 ] \le (1 - L(h))^{n} \le (1 - \epsilon)^n \le \exp^{-\epsilon n},  \]
        where we use the fact that $1 - x \le \exp(-x)$.

        \item Second, we want the above to hold simultaneously for all $h \in B$.
        We can apply the union bound to bound the probability of all bad events:
        \begin{align*} 
            \Pr[\exists h \in B : \hat L(h) = 0] &\le \sum_{h \in B} \Pr[ \hat L(h) = 0 ] \\
            & \le \abs{B} \exp(- \epsilon n) \\
            & \le \abs{\cH} \exp( - \epsilon n)
        \end{align*}
        By setting the above at most $\delta$, we conclude that $\epsilon$ must be at least $\frac {\log(\abs{\cH}) + \log(\delta^{-1}) } n$.
    \end{enumerate}
    This concludes the proof for learning finite, realizable hypothesis spaces.
\end{proof}

\paragraph{Takeaway:}
The proof of this result is elementary but illustrates an important pattern that will recur in more complex scenarios.
We are interested in the expected risk, but only have access to empirical risk to choose the ERM:
\begin{itemize}
    \item Step 1 (convergence): for a fixed $h$, show that $\hat L(h)$ is close to $L(h)$ with high probability
    \item Step 2 (uniform convergence): show that the above holds simultaneously for all hypotheses $h \in \cH$
\end{itemize}
However, the assumptions are restrictive.
There exists a perfect hypothesis (realizability). What happens when the problem is not realizable? To answer this, we introduce the tools of concentration estimates.

Second, the hypothesis class is finite. What happens when the number of hypotheses is infinite?
To answer this, we need better ways of measuring the ``size'' of a set -- leading to Rademacher complexity, VC, PAC-Bayes, etc.


\subsection{Using uniform convergence to reason about generalization (Lecture 2)}

We now give a high-level picture of the logic behind how we can use uniform convergence to reason about generalization (in the context of ERM).
We’d like to show that ERM’s excess risk is small:
\begin{align}
    \Pr\left[ L(\hat h_{\erm}) - \hat L(\hat h_{\erm}) \ge \epsilon \right] \le \delta
\end{align}
We can expand the excess risk as
\begin{align}
    L(\hat h) - L(h^{\star}) = \underbrace{\left(L(\hat h) - \hat L(\hat h) \right)}_{\text{Uniform convergence}} + \underbrace{\left( \hat L(\hat h) - \hat L(h^{\star}) \right)}_{\le 0} + \underbrace{\left( \hat L(h^{\star} )- L(h^{\star}) \right)}_{\text{Concentration}} 
\end{align}
We'll see how concentration estimates can be used to control this difference in the third part.
However, the same reasoning does not apply to the first part because the ERM $\hat h_{\erm}$ depends on the training examples $\hat L$.
In particular,
\begin{align}
    \hat L(\hat h_{\erm}) = \frac 1 n\sum_{i=1}^n \ell( \hat h_{\erm}(x_i), y_i ).
\end{align}
Due to the correlation, the above is not a sum of independent random variables.
The central thesis of uniform convergence is that if we could ensure that $L(h)$ and $\hat L(h)$ are close for all $h\in\cH$, then $L(\hat h_{\erm})$ must be close to $\hat L(\hat h_{\erm})$ as well.

In summary, our goal of deriving a uniform convergence result can be stated as
\begin{align}
    \Pr[L(\hat h_{\erm}) - L(h^{\star}) \ge \epsilon]
    \le \Pr\left[ \left(\sup_{h \in \cH} \abs{L(h) - \hat L(h)} \right) \ge \frac {\epsilon} 2 \right]
    \le \delta
\end{align}
In particular, the $1/2$ above comes from combining the error terms from the first and third parts together.
Put it in words, we'd like to upper bound the probability that the largest difference between the empirical risk and the expected risk is larger than $\epsilon /2$.

%\paragraph{Next lecture:}
%We'll talk about concentration estimates, which form the most fundamental techniques for dealing with IID random variables.
%Then we'll talk about a generalization bound for finite (not necessarily realizable) hypothesis classes.
%\textbf{Suggested reading:} Chapter 3.1-3.3 of Statistical learning theory lecture notes by Percy Liang.


\subsection{Concentration estimates (Lecture 3)}

\hl{Concentration inequalities} are powerful tools from probability theory that show the average of independent random variables will be concentrated around its expectation.

\paragraph{Example (mean estimation):}
Let $X_1, X_2, \dots, X_n$ be independent and identically distributed random variables with mean $\mu = \ex{X_i}$, for all $i = 1, 2, \dots, n$.
Define the empirical mean as \[ \hat \mu_n = \frac 1 n \sum_{i=1}^n X_i \]
How does $\hat \mu_n - \mu$ behave?

Three types of statements from probability:
\begin{itemize}
    \item \hl{Consistency:} by law of large numbers,
    \[ \hat \mu_n - \mu \rightarrow 0 \]

    \item \hl{Asymptotic normality:} let the variance of $X_i$ (for all $i$) be equal to $\sigma^2$, by the central limit theorem, we have
    \[ \sqrt{n} (\hat \mu_n - \mu) \sim \cN(0, \sigma^2) \]

    \item \hl{Tail estimates:} for our purpose, we would like to draw a statement of the following type
    \[ \Pr\left[ \bigabs{\hat \mu_n - \mu} \ge \epsilon \right] \le \delta \]
    For getting such tail estimates, we typically need to study the tail of a distribution, for instance, the tail of a Gaussian distribution, etc.
\end{itemize}

\paragraph{Markov's inequality:}
Let $Z \ge 0$ be a non-negative random variable. Then
\[ \Pr\left[Z \ge t \right] \le \frac {\ex{Z}} t \]

\paragraph{Proof:}
Since $Z$ is a non-negative quantity, we always have the condition that
\[ t \mathbbm{1}_{Z \ge t} \le Z \]
To see this, notice that if $Z \ge t$, then the above is true.
On the other hand, if $Z < t$, then the left-hand side above is zero, whereas $Z \ge 0$.
Next, take the expectation over $Z$ on both sides, we get
\[ \ex{t \mathbbm{1}_{Z \ge t}} \le \ex{Z} \Rightarrow \ex{\mathbbm{1}_{Z \ge t}} \le \frac {\ex{Z}} {t} \]
Notice that $\ex{\mathbbm{1}_{Z \ge t}} = \Pr[Z \ge t]$.
Thus, we have shown that Markov's inequality is true.

\paragraph{Chebyshev's inequality:}
Let $X$ be a random variable with mean equal to $\mu$. Then
\[ \Pr\left [ \bigabs{X - \mu} \ge \epsilon \right] \le \frac {\Var{X}} {\epsilon^2} \]

\paragraph{Proof:}
We will use Markov's inequality to get this result.
Let $Z = (X - \mu)^2$ and let $t = \epsilon^2$.
Notice that $Z \ge 0$. Thus, based on Markov's inequality
\[ \Pr\left[Z \ge t \right] \le \frac {\ex{Z}} {t} = \frac {\ex{(Z - \mu)^2}} t = \frac {\Var{Z}} t,  \]
which completes the proof.

\paragraph{Hoeffding's inequality:}
Let $Z_1, Z_2, \dots, Z_n$ be $n$ independent and identically distributed random variables drawn from a distribution with expectation $\mu$ and whose values are bounded from above by one.

Let $\hat\mu_n = \frac 1 n \sum_{i=1}^n Z_i$ denote the mean of the $n$ random variables.
Then, for any $\epsilon \in (0, 1)$, we have
\[ \Pr\left[ \bigabs{\hat\mu_n - \mu} > \epsilon \right] \le 2\exp(-2\epsilon^2 n) \]

The Hoeffding's inequality is a very powerful result when we work with the average of $n$ random variables.
Variants of this inequality are also called Chernoff bound.\footnote{\url{https://en.wikipedia.org/wiki/Chernoff_bound}}
Next, we shall see a proof through the use of moment generating functions (MGF).

\paragraph{Definition (Moment generating function):}
For a random variable $X$, its MGF is given by
\[ M_X(t) \define \ex{\exp(t X)} \]
One can also think of the MGF in terms of Taylor's expansion of $\exp(tX)$ as
\[ M_X(t) = 1 + t \ex{X} + \frac{t^2}{2} \ex{X^2} + \frac{t^3} 6 \ex{X^3} + \cdots  \]
Thus, the first moment is the mean of $X$.
The second moment is the variance of $X$ (assuming the mean of $X$ is zero).
And so on.

\paragraph{Property:} The MGF of the sum of two independent random variables $X_1$ and $X_2$ is the product of the MGF of $X_1$ and $X_2$, respectively.
\begin{itemize}
    \item To see this, notice that
    \[ M_{X + Y}(t) = \ex{e^{t(X + Y)}} = \ex{e^{tX} e^{tY}} = \ex{e^{tX}} \ex{e^{tY}} = M_X(t) M_Y(t) \]

    \item Here we have used that $X$ and $Y$ are independent, and hence $e^{tX}$ and $e^{tY}$ are independent, to conclude that $\ex{e^{tX} e^{tY}} = \ex{e^{tX}} \ex{e^{tY}}$
\end{itemize}

The high-level idea for showing the Hoeffding's inequality is obtained by applying Markov's inequality to $e^{tX}$ for some well-chosen value $t$.
From Markov's inequality, we can derive the following useful inequality: for any $t > 0$,
\[ \Pr[X \ge a] = \Pr\left[e^{tX} \ge e^{ta}\right] \le \frac {\ex{e^{tX}}} {e^{ta}}  \]
In particular,
\begin{align} \Pr[X \ge a] \le \min_{t > 0} \frac {\ex{e^{tX}}} {e^{ta}} \label{eq_tail_pos} \end{align}
Similarly, for any $t < 0$,
\[ \Pr[X \le a] = \Pr[e^{tX} \ge e^{ta}] \le \min_{t < 0} \frac {\ex{e^{tX}}} {e^{ta}} \]
Bounds for specific distributions are obtained by choosing appropriate values for $t$.

\subsubsection{Chernoff bounds for the sum of Poisson trials}

We now develop the most commonly used version of the Chernoff bound for the tail distribution of a sum of independent $0$-$1$ random variables, which are also known as Poisson trials.\footnote{Poisson trials differ from Poisson random variables.}

Let $X_1, \dots, X_n$ be a sequence of independent Poisson trials with $\Pr[X_i = 1] = p_i$.
Let $X = \sum_{i=1}^n X_i$, and let
\[ \mu = \ex{X} = \ex{\sum_{i=1}^n X_i} = \sum_{i=1}^n \ex{X_i} = \sum_{i=1}^n p_i \]

For a given $\delta > 0$, we are interested in bounds on $\Pr[X \ge (1 + \delta) \mu]$ and $\Pr[X \le (1 - \delta) \mu]$, that is, the probability that $X$ deviates from its expectation $\mu$ by $\delta \mu$ or more.
To develop a Chernoff bound we need to compute the moment generating function of $X$.
We start with the MGF of each $X_i$:
\begin{align*}
    M_{X_i} (t) = \ex{e^{t X_i}} = p_i e^t + (1 - p_i) &= 1 + p_i (e^t - 1) \\
    &\le e^{p_i(e^t - 1)},
\end{align*}
where in the last step we have used the fact that, for any $y$, $1 + y \le e^y$.
Since the $X_i$'s are independent from each other, we take the product of the $n$ MGF to obtain
\begin{align*}
    M_X(t) = \prod_{i=1}^n M_{X_i}(t) &\le \prod_{i=1}^n e^{p_i(e^t - 1)} \\
    &= \exp\left(\sum_{i=1}^n p_i (e^t - 1) \right) = e^{(e^t - 1) \mu}
\end{align*}
Based on this result, we now apply Markov's inequality: for any $t > 0$ we have
\begin{align*}
    \Pr[X \ge (1 + \delta) \mu] &= \Pr[e^{tX} \ge e^{t(1 + \delta) \mu}] \\
    &\le \frac {\ex{e^{tX}}} {e^{t(1 + \delta) \mu}} \\
    &\le \frac {e^{(e^t - 1) \mu}} {e^{t(1 + \delta) \mu}}
\end{align*}
For any $\delta >0$, we can set $t = \ln(1 + \delta) > 0$ to get
\[ \Pr[X \ge (1 + \delta) \mu] \le \left( \frac {e^{\delta}} {(1 + \delta)^{1 + \delta}} \right)^{\mu} \]
For $0 < \delta \le 1$, we need to show that
\[ \frac {e^{\delta}} {(1 + \delta)^{1 + \delta}} \le e^{-\delta^2 / 3} \]
Taking the logarithm of both sides, we obtain
\[ f(\delta) = \delta - (1 + \delta) \ln(1 + \delta) + \frac {\delta^2} 3 \le 0 \]
Computing the derivatives of $f(\delta)$, we have:
\begin{align*}
    f'(\delta) = 1 - \frac {1 + \delta}{1 + \delta} - \ln(1 + \delta) + \frac {2 \delta} 3\\
    f''(\delta) = - \frac 1 {1 + \delta} + \frac 2 3
\end{align*}
We see that the second derivative is less than zero if $\delta < 1/2$ and it is positive otherwise.
Hence, $f'(\delta)$ first decreases and then increases in the interval $[0, 1]$.
Since $f'(0) = 0$ and $f'(1) < 0$, we conclude that $f'(\delta) \le 0$ in the interval $[0, 1]$.
Since $f(0) = 0$, it follows that $f(\delta) \le 0$, which shows that for any $\delta$ between $0$ and $1$, we have
\[ \Pr[X \ge (1 + \delta)\mu] \le e^{-\mu\delta^2 / 3}\]

\subsubsection{Example: Coin flips}

Let $X$ be the number of heads in a sequence of $n$ independent fair coin flipts.
Applying the Chernoff bound, we have
\[ \Pr\left[ \bigabs{X - \frac n 2} \ge \frac 1 2 \sqrt{6n\ln n} \right] \le 2\exp\left( - \frac 1 3 \frac n 2  \frac {6\ln n}{n}\right) = \frac 2 n \]
This demonstrates that the concentration of the number of heads around the mean $n/2$ is very tight.
Most of the time, the deviations from the mean are on the order of $O(\sqrt {n \ln n})$.

\subsubsection{Gaussian random variables}

In the next example, we look at the MGF of Gaussian random variables.
Let $X \sim \cN(0, \sigma^2)$. Then, $M_X(t) = e^{\sigma^2 t^2 / 2}$.
To derive this, we use the definition of Gaussian probability density:
\begin{align*}
    M_X(t) = \ex{e^{tX}} &= \int \frac 1 {\sqrt{2\pi\sigma^2}} \exp\left( - \frac {x^2 - 2\sigma^2 tx} {2\sigma^2} \right) dx \\
    &= \int \frac 1 {\sqrt{2 \pi\sigma^2}} \exp\left(- \frac {(x - \sigma^2 t)^2 - \sigma^4 t^2} {2\sigma^2} \right) dx \\
    &= \exp\left( \frac {\sigma^2 t^2} 2 \right)
\end{align*}

Based on the above MGF, we can derive a tail bound by plugging the form of MGF to equation \eqref{eq_tail_pos} to get:
\begin{align*}
    \Pr[X \ge \epsilon] \le \inf_t \exp\left(\frac {\sigma^2 t^2} 2 - t\epsilon \right)
\end{align*}
The infimum of the RHS is attained by setting $e = \frac{\epsilon} {\sigma^2}$, yielding:
\[ \Pr[X \ge \epsilon] \le \exp\left( -\frac{ \epsilon^2} {2\sigma^2} \right) \]

What about non-Gaussian random variables? Notice that the bound still holds if we replace $M_X(t)$ with an upper bound.
This motivates the following definition.

\paragraph{Sub-Gaussian:} A mean zero random variable $X$ is \hl{sub-Gaussian} with parameter $\sigma^2$ if its MGF is bounded as follows:
\begin{align}
    M_X(t) \le \exp\left(\frac{\sigma^2 t^2}2\right)
\end{align}
It follows that for sub-Gaussian $X$, we again have that $\Pr[X \ge \epsilon] \le \exp(-\epsilon^2 /(2\sigma^2))$.

\paragraph{Next lecture:} Based on the concentration estimates we have established, next time we'll talk about how to get a generalization bound for learning finite hypothesis classes. We'll then start to talk about the Rademacher complexity, which is perhaps the most fundamental tool for deriving generalization bounds. {\bf Suggested readings:} Chapter 3.5 of \cite{liang2016cs229t}, and Chapter 3 of \cite{mitzenmacher2017probability}.
