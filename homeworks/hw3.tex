\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{epsfig}
\def\shownotes{1}
\usepackage{source_files/macro}
\usepackage{url}
\usepackage{mdframed}
\usepackage[noend,noline]{algorithm2e}
%\usepackage{enumerate,enumitem}
\SetEndCharOfAlgoLine{}
\SetArgSty{}
\SetKwBlock{Repeat}{repeat}{}

\everymath=\expandafter{\the\everymath\displaystyle}
%%\usepackage{psfig}
\DeclareMathSizes{24}{24}{24}{24}
\newcommand{\lecture}[2]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf CS7140: Advanced Machine Learning} \hfill Spring 2025}
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #1  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em Instructor: Ryan Zhang  \hfill Due: #2} }
    }
  }
  \end{center}
  \vspace*{4mm}
}
%\newcommand{\dim}{\textup{dim}}


\newcommand*{\diffdchar}{d}    % or {ⅆ}, or {\mathrm{d}}, or whatever standard you’d like to adhere to
\newcommand*{\dd}{\mathop{\diffdchar\!}}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

\newcommand{\E}{\mathbb{E}}





\usepackage{amsmath, amssymb}
\usepackage{fullpage}
\pagestyle{empty}
\def\pp{\par\noindent}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.2}
\newcommand{\problem}[1]{ \bigskip \pp \textbf{Problem #1}\par}
\newcommand{\solution}{\textit{Solution:}\par}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\bbZ}    {\mathbb{Z}}
\newcommand{\bbQ}    {\mathbb{Q}}
\newcommand{\bbN}    {\mathbb{N}}
\newcommand{\bbB}    {\mathbb{B}}
\newcommand{\bbR}    {\mathbb{R}}
\newcommand{\bbC}    {\mathbb{C}}
\newcommand{\calP}   {{\cal{P}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\lecture{Problem Set 3}{\textbf{Mar 29, 2025, 11:59pm}}

\textbf{Policy:} We encourage discussions and collaborations on homework. You should write up the solution independently and remember to mention any fellow students you collaborated with. There are up to three late days for all the problem sets and project submissions.
After that, the grade depreciates by 20\% for every extra day.
Late submissions are treated on a case-by-case basis. Please reach out to the instructor at \texttt{ho.zhang@northeastern.edu} to discuss.
All homework submissions are subject to the Northeastern University Honor Code.

\textbf{Submission:} We will use Gradescope for the homework submissions. Please submit your written solutions to Gradescope.
Login to Gradescope through Canvas using your northeastern.edu account.
For code submission, please print out the code file and attach it to the PDF solution file.
We strongly recommend that you write up your solution in LaTeX. For reference, you can find the LaTex source code at \url{https://github.com/hongyangsg/cs7140/tree/main/homeworks}.

\textbf{Length of submissions:} Include as much of the calculations needed to understand the answer. After solving the problem, try to identify the main steps taken and critical points of proof and include them as a rule of thumb.


\newpage
\paragraph{Problem 1 (20 points)}
Let $f: \mathbb{R}^n\rightarrow \mathbb{R}$ be a convex function taking as input a vector $x\in \mathbb{R}^n$. The Hessian matrix $\mathbf{H}$ of $f$ is a square matrix in $\real^{n\times n}$ with the $(i,j)$-th entry of defined as:
\[ \mathbf{H}[f]_{i,j} = \frac{\partial^2 f(x)}{\partial x_i \partial x_j}. \]
Show that the Hessian matrix $\mathbf{H}[f]$ is positive semidefinite.

\newpage
\paragraph{Problem 2 (20 points).}
Suppose $x_1, x_2, \cdots, x_n$ are all drawn i.i.d. from some unknown distribution $p^{\star}$ supported in the $d$-dimensional space $\mathbb{R}^d$.
Furthermore, for any $i = 1,2,\dots,n$, suppose the largest coordinate of $x_i$ are all bounded: $||x_i||_{\infty}\leq B)$.
Recall that the covariance matrix of a distibution $p^{\star}$ is defined as
$$\Sigma = \mathbb{E}_{x\sim p^{\star}}\Big{[}(x-\mathbb{E}_{x\sim p^{\star}}[x])(x-\mathbb{E}_{x\sim p^{\star}}[x])^T\Big{]}.$$
Assume that $p^{\star}$ is centered at zero: $\mathbb{E}_{x\sim p}[x] = 0$.
Define the sample covariance matrix
$$\hat{\Sigma} = \frac{1}{n}\sum_{i = 1}^n x_ix_i^T.$$
For any fixed $\epsilon > 0$, show that
$$\mathbb{P}\Big{[}\max_{1\leq i,j\leq d}|\hat{\Sigma}_{ij} - \Sigma_{ij}|\geq \epsilon\Big{]}\leq 2d^2\exp \left(-\frac{n\epsilon^2}{2B^4}\right).$$


\newpage
\paragraph{Problem 3 (20 points).}
Consider ReLU neural networks for approximating one-dimensional functions on $[0, 1]$.
Prove that any piecewise linear function can be represented by a two-layer ReLU network.

\newpage
\paragraph{Problem 4 (40 points).}
For each of the following questions, provide $2$-$3$ sentences of explanation (8 points each).
\begin{itemize}
    \item What is the relationship between the moment generating function of a random variable, and the moments of the random variable?

    \item Describe the high-level approach to proving a Chernoff-style bound. (A few sentences are sufficient.)

    \item Why is the above approach often much easier to apply to a sum of independent random variables, rather than dependent ones?

    \item What is the pseudoinverse of matrix? And when should you use pseudoinverse or the inverse?

    \item What is the hypothesis of benign overfitting? In the context of overparameterized linear regression, when do we expect benign overfitting to occur?

    \item (BONUS) Let $\mathcal{H}$ denote a hypothesis class that consists of all ellipsoids:
    $$\mathcal{H} = \Big\{x\mapsto\mathbb{I}\big(x^T\Sigma x\geq 1\big):\Sigma\in\mathbb{R}^{d\times d},~ \Sigma\succeq 0\Big\}.$$
    Show that the VC dimension of $\mathcal{H}$ is upper bounded by  $\mathcal{O}(d^2)$.
    Can you briefly explain the key proof steps? Again, a few sentences are sufficient.
    
    [Note: the notation $\Sigma\succeq 0$ means that $\Sigma$ is a positive semidefinite matrix whose eigenvalues are all non-negative.]
\end{itemize}
\end{document}